name: Convert Phi-4 to Core ML

on:
  push:
    branches: [main]
    paths:
      - 'scripts/fine_tune_phi4_quebec.py'
      - 'scripts/convert_to_coreml.py'
      - '.github/workflows/convert-coreml-models.yml'
  workflow_dispatch:
    inputs:
      model_variant:
        description: 'Model variant to convert'
        required: true
        default: 'phi-4-mini-reasoning'
        type: choice
        options:
          - 'phi-4-mini-reasoning'
          - 'phi-4-mini-reasoning-quebec'
          - 'phi-4-mini-instruct'
      quantization:
        description: 'Quantization level'
        required: true
        default: 'int4'
        type: choice
        options:
          - 'none'
          - 'int8'
          - 'int4'
      optimize_for:
        description: 'Optimization target'
        required: true
        default: 'neural_engine' 
        type: choice
        options:
          - 'neural_engine'
          - 'gpu'
          - 'cpu'

env:
  PYTHON_VERSION: '3.11'
  PYTORCH_VERSION: '2.1.0'
  COREMLTOOLS_VERSION: '7.1'

jobs:
  # Prepare Model Conversion Environment
  setup-conversion-env:
    runs-on: macos-latest
    outputs:
      cache-key: ${{ steps.cache-key.outputs.key }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Generate Cache Key
        id: cache-key
        run: |
          echo "key=coreml-env-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('scripts/requirements-coreml.txt') }}" >> $GITHUB_OUTPUT

      - name: Cache Python Environment
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/Library/Caches/pip
          key: ${{ steps.cache-key.outputs.key }}

      - name: Install Core ML Conversion Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch==${{ env.PYTORCH_VERSION }}
          pip install coremltools==${{ env.COREMLTOOLS_VERSION }}
          pip install transformers accelerate bitsandbytes
          pip install numpy scipy pillow
          echo "✅ Core ML conversion environment ready"

  # Convert Phi-4 Models to Core ML
  convert-phi4-models:
    runs-on: macos-latest
    needs: setup-conversion-env
    strategy:
      matrix:
        model: 
          - name: 'phi-4-mini-reasoning'
            path: 'microsoft/phi-4'
            config: 'reasoning'
          - name: 'phi-4-mini-reasoning-quebec'
            path: 'microsoft/phi-4'
            config: 'quebec'
        quantization: ['int4', 'int8']
        compute_unit: ['neural_engine', 'gpu']
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Restore Python Cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/Library/Caches/pip
          key: ${{ needs.setup-conversion-env.outputs.cache-key }}

      - name: Install Dependencies
        run: |
          pip install torch==${{ env.PYTORCH_VERSION }}
          pip install coremltools==${{ env.COREMLTOOLS_VERSION }}
          pip install transformers accelerate bitsandbytes

      - name: Create Core ML Conversion Script
        run: |
          cat > scripts/convert_to_coreml.py << 'EOF'
          #!/usr/bin/env python3
          """
          Phi-4-mini-reasoning to Core ML Conversion Pipeline
          
          This script converts Phi-4 models to optimized Core ML format
          for on-device inference with Apple Neural Engine acceleration.
          """
          
          import os
          import sys
          import argparse
          import logging
          from pathlib import Path
          
          import torch
          import coremltools as ct
          from transformers import AutoTokenizer, AutoModelForCausalLM
          import numpy as np
          
          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger(__name__)
          
          def convert_phi4_to_coreml(
              model_name: str,
              output_path: str,
              quantization: str = "int4",
              compute_units: str = "neural_engine",
              max_length: int = 512
          ):
              """Convert Phi-4 model to Core ML format."""
              
              logger.info(f"🚀 Starting conversion: {model_name}")
              logger.info(f"   Quantization: {quantization}")
              logger.info(f"   Compute Units: {compute_units}")
              logger.info(f"   Max Length: {max_length}")
              
              # Load model and tokenizer
              logger.info("📥 Loading Phi-4 model and tokenizer...")
              tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
              model = AutoModelForCausalLM.from_pretrained(
                  model_name,
                  torch_dtype=torch.float16,
                  trust_remote_code=True,
                  device_map="cpu"  # Keep on CPU for conversion
              )
              
              # Prepare for conversion
              model.eval()
              
              # Create sample input for tracing
              sample_input = tokenizer(
                  "What is 2 + 2?",
                  return_tensors="pt",
                  max_length=max_length,
                  padding="max_length",
                  truncation=True
              )
              
              logger.info("🔄 Converting to Core ML...")
              
              # Convert to Core ML
              with torch.no_grad():
                  traced_model = torch.jit.trace(
                      model,
                      (sample_input["input_ids"], sample_input["attention_mask"])
                  )
              
              # Configure Core ML conversion
              compute_unit_map = {
                  "neural_engine": ct.ComputeUnit.CPU_AND_NE,
                  "gpu": ct.ComputeUnit.CPU_AND_GPU,
                  "cpu": ct.ComputeUnit.CPU_ONLY
              }
              
              # Convert to Core ML
              coreml_model = ct.convert(
                  traced_model,
                  inputs=[
                      ct.TensorType(
                          name="input_ids",
                          shape=sample_input["input_ids"].shape,
                          dtype=np.int32
                      ),
                      ct.TensorType(
                          name="attention_mask", 
                          shape=sample_input["attention_mask"].shape,
                          dtype=np.int32
                      )
                  ],
                  compute_units=compute_unit_map[compute_units],
                  minimum_deployment_target=ct.target.iOS16,
                  convert_to="mlprogram"
              )
              
              # Apply quantization
              if quantization != "none":
                  logger.info(f"⚡ Applying {quantization} quantization...")
                  if quantization == "int4":
                      coreml_model = ct.optimize.coreml.linear_quantize_weights(
                          coreml_model, mode="linear_symmetric", dtype=np.int4
                      )
                  elif quantization == "int8":
                      coreml_model = ct.optimize.coreml.linear_quantize_weights(
                          coreml_model, mode="linear_symmetric", dtype=np.int8
                      )
              
              # Add metadata
              coreml_model.short_description = f"Phi-4-mini-reasoning optimized for {compute_units}"
              coreml_model.author = "Vibecode Advanced Agent System"
              coreml_model.license = "MIT"
              coreml_model.version = "1.0.0"
              
              # Save model
              output_file = f"{output_path}/{model_name.replace('/', '_')}_{quantization}_{compute_units}.mlpackage"
              os.makedirs(output_path, exist_ok=True)
              
              logger.info(f"💾 Saving Core ML model: {output_file}")
              coreml_model.save(output_file)
              
              # Generate model info
              model_info = {
                  "model_name": model_name,
                  "quantization": quantization,
                  "compute_units": compute_units,
                  "max_length": max_length,
                  "file_size_mb": os.path.getsize(output_file) / (1024 * 1024),
                  "output_path": output_file
              }
              
              logger.info("✅ Conversion completed successfully!")
              logger.info(f"   File size: {model_info['file_size_mb']:.1f} MB")
              
              return model_info
          
          if __name__ == "__main__":
              parser = argparse.ArgumentParser()
              parser.add_argument("--model", required=True)
              parser.add_argument("--output", required=True)
              parser.add_argument("--quantization", default="int4")
              parser.add_argument("--compute-units", default="neural_engine")
              parser.add_argument("--max-length", type=int, default=512)
              
              args = parser.parse_args()
              
              convert_phi4_to_coreml(
                  args.model,
                  args.output,
                  args.quantization,
                  args.compute_units,
                  args.max_length
              )
          EOF

      - name: Convert Model to Core ML
        run: |
          mkdir -p models/coreml
          python scripts/convert_to_coreml.py \
            --model "${{ matrix.model.path }}" \
            --output "models/coreml" \
            --quantization "${{ matrix.quantization }}" \
            --compute-units "${{ matrix.compute_unit }}" \
            --max-length 512

      - name: Validate Core ML Model
        run: |
          python -c "
          import coremltools as ct
          import glob
          
          models = glob.glob('models/coreml/*.mlpackage')
          for model_path in models:
              print(f'🔍 Validating: {model_path}')
              model = ct.models.MLModel(model_path)
              print(f'  ✅ Model loaded successfully')
              print(f'  📊 Spec version: {model._spec.specificationVersion}')
              print(f'  🎯 Compute units: {model.compute_unit}')
          "

      - name: Generate Model Metadata
        run: |
          python -c "
          import json
          import glob
          import os
          from datetime import datetime
          
          models = glob.glob('models/coreml/*.mlpackage')
          metadata = {
              'conversion_date': datetime.now().isoformat(),
              'model_variant': '${{ matrix.model.name }}',
              'quantization': '${{ matrix.quantization }}',
              'compute_unit': '${{ matrix.compute_unit }}',
              'models': []
          }
          
          for model_path in models:
              size_mb = os.path.getsize(model_path) / (1024 * 1024)
              metadata['models'].append({
                  'path': model_path,
                  'size_mb': round(size_mb, 2),
                  'name': os.path.basename(model_path)
              })
          
          with open('models/coreml/metadata.json', 'w') as f:
              json.dump(metadata, f, indent=2)
          
          print('✅ Model metadata generated')
          "

      - name: Archive Core ML Models
        uses: actions/upload-artifact@v4
        with:
          name: coreml-models-${{ matrix.model.name }}-${{ matrix.quantization }}-${{ matrix.compute_unit }}
          path: models/coreml/
          retention-days: 30

  # Test Core ML Models
  test-coreml-models:
    runs-on: macos-latest
    needs: convert-phi4-models
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download All Core ML Models
        uses: actions/download-artifact@v4
        with:
          pattern: coreml-models-*
          path: models/downloaded/
          merge-multiple: true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Testing Dependencies
        run: |
          pip install coremltools transformers torch numpy

      - name: Create Core ML Test Script
        run: |
          cat > test_coreml_models.py << 'EOF'
          import coremltools as ct
          import numpy as np
          import glob
          import time
          
          def test_coreml_model(model_path):
              print(f"🧪 Testing: {model_path}")
              
              try:
                  # Load model
                  model = ct.models.MLModel(model_path)
                  
                  # Create sample input
                  sample_input = {
                      "input_ids": np.array([[1, 2, 3, 4, 5]], dtype=np.int32),
                      "attention_mask": np.array([[1, 1, 1, 1, 1]], dtype=np.int32)
                  }
                  
                  # Test inference
                  start_time = time.time()
                  prediction = model.predict(sample_input)
                  inference_time = time.time() - start_time
                  
                  print(f"  ✅ Inference successful")
                  print(f"  ⏱️  Inference time: {inference_time:.3f}s")
                  print(f"  📊 Output shape: {next(iter(prediction.values())).shape}")
                  
                  return True
                  
              except Exception as e:
                  print(f"  ❌ Test failed: {e}")
                  return False
          
          # Test all models
          models = glob.glob("models/downloaded/**/*.mlpackage", recursive=True)
          print(f"Found {len(models)} Core ML models to test")
          
          passed = 0
          failed = 0
          
          for model_path in models:
              if test_coreml_model(model_path):
                  passed += 1
              else:
                  failed += 1
          
          print(f"\n📊 Test Results:")
          print(f"  ✅ Passed: {passed}")
          print(f"  ❌ Failed: {failed}")
          
          if failed > 0:
              exit(1)
          EOF

      - name: Test Core ML Models
        run: python test_coreml_models.py

      - name: Generate Test Report
        run: |
          echo "# Core ML Model Test Report" > test-report.md
          echo "" >> test-report.md
          echo "**Test Date:** $(date)" >> test-report.md
          echo "**Models Tested:** $(find models/downloaded -name "*.mlpackage" | wc -l)" >> test-report.md
          echo "" >> test-report.md
          echo "## Model Details" >> test-report.md
          
          find models/downloaded -name "*.mlpackage" | while read model; do
            size=$(du -h "$model" | cut -f1)
            echo "- **$(basename "$model"):** $size" >> test-report.md
          done

      - name: Archive Test Report
        uses: actions/upload-artifact@v4
        with:
          name: coreml-test-report
          path: test-report.md

  # Create Release with Core ML Models
  create-model-release:
    runs-on: ubuntu-latest
    needs: [test-coreml-models]
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          path: release-assets/

      - name: Prepare Release Assets
        run: |
          mkdir -p release/
          find release-assets -name "*.mlpackage" -exec cp -r {} release/ \;
          find release-assets -name "metadata.json" -exec cp {} release/ \;
          
          # Create release notes
          cat > release/RELEASE_NOTES.md << 'EOF'
          # Phi-4-mini-reasoning Core ML Models
          
          This release contains optimized Core ML models for on-device inference:
          
          ## Models Included
          - **phi-4-mini-reasoning** (INT4/INT8 quantized)
          - **phi-4-mini-reasoning-quebec** (Québécois French fine-tuned)
          
          ## Optimization Targets
          - 🧠 **Neural Engine** - Maximum performance on Apple Silicon
          - 🎮 **GPU** - Balanced performance and compatibility
          
          ## Usage
          1. Download the appropriate `.mlpackage` for your needs
          2. Add to your iOS project bundle
          3. Load with `nativePhi4LLM.loadModel(path)`
          
          ## Performance
          - **Inference Speed:** ~50-100 tokens/second on Neural Engine
          - **Memory Usage:** ~500MB-1GB depending on quantization
          - **Compatibility:** iOS 16+ required
          EOF

      - name: Create Release
        uses: softprops/action-gh-release@v1
        with:
          tag_name: coreml-models-v${{ github.run_number }}
          name: Core ML Models Release v${{ github.run_number }}
          body_path: release/RELEASE_NOTES.md
          files: |
            release/*.mlpackage
            release/metadata.json
          draft: false
          prerelease: false
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}