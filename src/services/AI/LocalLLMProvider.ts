import { LLMConfig, LLMError } from './LLMProvider';
import { BaseMemoryProvider } from './BaseMemoryProvider';

/**
 * Local LLM Provider Implementation (Placeholder) with Memory Integration
 * 
 * This is a placeholder implementation that returns mocked responses.
 * In a production environment, this would connect to a local LLM instance
 * such as Ollama, LlamaFile, or other on-device inference engines.
 */
export class LocalLLMProvider extends BaseMemoryProvider {
  public readonly providerId = 'local';
  private mockResponses: string[];
  private responseIndex = 0;

  constructor(config: LLMConfig = {}) {
    const defaultConfig = {
      model: 'local-llm-mock',
      temperature: 0.7,
      maxTokens: 2048,
      timeout: 5000,
      memoryEnabled: true,
      maxMemoryContext: 3,
      ...config,
    };

    super(defaultConfig);

    // Mock responses for testing
    this.mockResponses = [
      "Hello! I'm a local AI assistant running on your device. This is a mocked response for testing purposes.",
      "I understand your request. As a local LLM, I can provide responses without requiring an internet connection.",
      "This is another example response from the local LLM provider. In a real implementation, this would be generated by an on-device model.",
      "Local LLMs offer privacy benefits since your data doesn't leave your device. This response simulates that capability.",
      "I'm processing your input locally. This mock response demonstrates the local LLM provider interface.",
    ];
  }

  async isAvailable(): Promise<boolean> {
    // Simulate availability check with a small delay
    await new Promise(resolve => setTimeout(resolve, 100));
    return true; // Always available in mock mode
  }

  protected async getCoreStreamResponse(prompt: string): Promise<ReadableStream> {
    const response = this.getNextMockResponse(prompt);
    
    return new ReadableStream({
      start(controller) {
        // Simulate streaming by sending chunks with delays
        const words = response.split(' ');
        let wordIndex = 0;

        const sendNextChunk = () => {
          if (wordIndex < words.length) {
            const chunk = wordIndex === 0 ? words[wordIndex] : ' ' + words[wordIndex];
            controller.enqueue(new TextEncoder().encode(chunk));
            wordIndex++;
            
            // Simulate realistic typing speed (100-300ms per word)
            const delay = Math.random() * 200 + 100;
            setTimeout(sendNextChunk, delay);
          } else {
            controller.close();
          }
        };

        // Start streaming after a brief delay
        setTimeout(sendNextChunk, 200);
      },
    });
  }

  protected async getCoreResponse(prompt: string): Promise<string> {
    // Simulate processing time
    await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 2000));
    
    return this.getNextMockResponse(prompt);
  }

  private getNextMockResponse(prompt: string): string {
    // Check if this is a memory-enhanced prompt
    const isMemoryEnhanced = prompt.includes('Relevant conversation history:');
    
    // Create a contextual response based on the prompt
    const lowerPrompt = prompt.toLowerCase();
    
    if (lowerPrompt.includes('hello') || lowerPrompt.includes('hi')) {
      return isMemoryEnhanced 
        ? "Hello! I can see our previous conversations and I'm ready to continue helping you."
        : "Hello! I'm your local AI assistant. How can I help you today?";
    }
    
    if (lowerPrompt.includes('weather')) {
      return "I'm a local LLM and don't have access to real-time weather data. In a full implementation, I could integrate with local weather APIs.";
    }
    
    if (lowerPrompt.includes('time') || lowerPrompt.includes('date')) {
      const now = new Date();
      return `The current time is ${now.toLocaleTimeString()} on ${now.toLocaleDateString()}. This is provided by the local LLM mock.`;
    }
    
    if (lowerPrompt.includes('local') || lowerPrompt.includes('privacy')) {
      return "As a local LLM, I process everything on your device, ensuring your privacy and data security. No information is sent to external servers.";
    }

    if (isMemoryEnhanced) {
      return "I can see relevant context from our previous conversations. This helps me provide more informed and contextual responses based on our chat history.";
    }
    
    // Default to cycling through mock responses
    const response = this.mockResponses[this.responseIndex];
    this.responseIndex = (this.responseIndex + 1) % this.mockResponses.length;
    
    // Extract the actual user input from memory-enhanced prompts
    const actualPrompt = isMemoryEnhanced 
      ? prompt.split('Current user input: ')[1] || prompt
      : prompt;
    
    const truncatedPrompt = actualPrompt.substring(0, 50) + (actualPrompt.length > 50 ? '...' : '');
    return `${response}\n\n(Responding to: "${truncatedPrompt}")`;
  }



  /**
   * Simulate connection to local LLM (for future implementation)
   */
  async connect(): Promise<void> {
    // In a real implementation, this would:
    // - Check if local LLM is installed
    // - Initialize connection to local inference engine
    // - Load the specified model
    await new Promise(resolve => setTimeout(resolve, 500));
    console.log('Mock: Connected to local LLM');
  }

  /**
   * Simulate disconnection from local LLM
   */
  async disconnect(): Promise<void> {
    await new Promise(resolve => setTimeout(resolve, 200));
    console.log('Mock: Disconnected from local LLM');
  }

  /**
   * Get available local models (mock)
   */
  async getAvailableModels(): Promise<string[]> {
    return [
      'local-llm-mock',
      'llama-3.2-1b',
      'phi-3-mini',
      'gemma-2b',
      'mistral-7b-instruct',
    ];
  }
}